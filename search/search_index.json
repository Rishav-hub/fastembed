{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\u26a1\ufe0f What is FastEmbed?","text":"<p>FastEmbed is an easy to use -- lightweight, fast, Python library built for retrieval embedding generation. </p> <p>The default embedding supports \"query\" and \"passage\" prefixes for the input text. The default model is Flag Embedding, which is top of the MTEB leaderboard. </p> <p>Advanced user? Skip ahead to Retrieval with FastEmbed</p> <p>To install the FastEmbed library, pip works: </p> <pre><code>pip install fastembed\n</code></pre>"},{"location":"#usage","title":"\ud83d\udcd6 Usage","text":"<pre><code>from fastembed.embedding import DefaultEmbedding\n\ndocuments: List[str] = [\n    \"passage: Hello, World!\",\n    \"query: Hello, World!\", # these are two different embedding\n    \"passage: This is an example passage.\",\n    # You can leave out the prefix but it's recommended\n    \"fastembed is supported by and maintained by Qdrant.\" \n]\nembedding_model = DefaultEmbedding() \nembeddings: List[np.ndarray] = list(embedding_model.embed(documents))\n</code></pre>"},{"location":"#under-the-hood","title":"\ud83d\ude92 Under the hood","text":""},{"location":"#why-fast","title":"Why fast?","text":"<p>It's important we justify the \"fast\" in FastEmbed. FastEmbed is fast because:</p> <ol> <li>Quantized model weights</li> <li>ONNX Runtime which allows for inference on CPU, GPU, and other dedicated runtimes</li> </ol>"},{"location":"#why-light","title":"Why light?","text":"<ol> <li>No hidden dependencies on PyTorch or TensorFlow via Huggingface Transformers</li> </ol>"},{"location":"#why-accurate","title":"Why accurate?","text":"<ol> <li>Better than OpenAI Ada-002</li> <li>Top of the Embedding leaderboards e.g. MTEB</li> </ol>"},{"location":"Getting%20Started/","title":"Getting Started","text":"<pre><code>!pip install fastembed --upgrade # Install fastembed\n</code></pre> <p>Make the necessary imports, initialize the <code>Embedding</code> class, and embed your data into vectors:</p> <pre><code>from typing import List\nimport numpy as np\nfrom fastembed.embedding import DefaultEmbedding\n\n# Example list of documents\ndocuments: List[str] = [\n    \"Hello, World!\",\n    \"This is an example document.\",\n    \"fastembed is supported by and maintained by Qdrant.\",\n]\n# Initialize the DefaultEmbedding class with the desired parameters\nembedding_model = DefaultEmbedding(model_name=\"BAAI/bge-small-en\", max_length=512)\nembeddings: List[np.ndarray] = list(\n    embedding_model.embed(documents)\n)  # notice that we are casting the generator to a list\n\nprint(embeddings[0].shape)\n</code></pre> <pre><code>from typing import List\nimport numpy as np\nfrom fastembed.embedding import DefaultEmbedding as Embedding\n</code></pre> <p>Notice that we are using the DefaultEmbedding -- which is a quantized, state of the Art Flag Embedding model which beats OpenAI's Embedding by a large margin. </p> <pre><code># Example list of documents\ndocuments: List[str] = [\n    \"passage: Hello, World!\",\n    \"query: Hello, World!\",  # these are two different embedding\n    \"passage: This is an example passage.\",\n    # You can leave out the prefix but it's recommended\n    \"fastembed is supported by and maintained by Qdrant.\",\n]\n</code></pre> <pre><code>embedding_model = DefaultEmbedding()\n</code></pre> <pre><code>embeddings: List[np.ndarray] = list(\n    embedding_model.embed(documents)\n)  # notice that we are casting the generator to a list\n</code></pre> <p>You can print the shape of the embeddings to understand their dimensions. Typically, the shape will indicate the number of dimensions in the vector.</p> <pre><code>print(embeddings[0].shape)  # (384,) or similar output\n</code></pre>"},{"location":"Getting%20Started/#getting-started","title":"\ud83d\udeb6\ud83c\udffb\u200d\u2642\ufe0f Getting Started","text":"<p>Here you will learn how to use the fastembed package to embed your data into a vector space. The package is designed to be easy to use and fast. It is built on top of the ONNX standard, which allows for fast inference on a variety of hardware (called Runtimes in ONNX). </p>"},{"location":"Getting%20Started/#quick-start","title":"Quick Start","text":"<p>The fastembed package is designed to be easy to use. The main class is the <code>Embedding</code> class. It takes a list of strings as input and returns a list of vectors as output. The <code>Embedding</code> class is initialized with a model file.</p>"},{"location":"Getting%20Started/#lets-think-step-by-step","title":"Let's think step by step","text":""},{"location":"Getting%20Started/#setup","title":"Setup","text":"<p>Importing the required classes and modules:</p>"},{"location":"Getting%20Started/#prepare-your-documents","title":"Prepare your Documents","text":"<p>You can define a list of documents that you'd like to embed. These can be sentences, paragraphs, or even entire documents. </p>"},{"location":"Getting%20Started/#format-of-the-document-list","title":"Format of the Document List","text":"<ol> <li>List of Strings: Your documents must be in a list, and each document must be a string</li> <li>For Retrieval Tasks: If you're working with queries and passages, you can add special labels to them:</li> <li>Queries: Add \"query:\" at the beginning of each query string</li> <li>Passages: Add \"passage:\" at the beginning of each passage string</li> </ol>"},{"location":"Getting%20Started/#load-the-embedding-model-weights","title":"Load the Embedding Model Weights","text":"<p>Next, initialize the Embedding class with the desired parameters. Here, \"BAAI/bge-small-en\" is the pre-trained model name, and max_length=512 is the maximum token length for each document.</p> <p>This will download the model weights, decompress to directory <code>local_cache</code> and load them into the Embedding class.</p>"},{"location":"Getting%20Started/#initialize-defaultembedding","title":"Initialize DefaultEmbedding","text":"<p>We will initialize Flag Embeddings with the model name and the maximum token length. That is the DefaultEmbedding class with the model name \"BAAI/bge-small-en\" and max_length=512.</p>"},{"location":"Getting%20Started/#embed-your-documents","title":"Embed your Documents","text":"<p>Use the embed method of the embedding model to transform the documents into a List of np.array. The method returns a generator, so we cast it to a list to get the embeddings.</p>"},{"location":"examples/HF_vs_FastEmbed/","title":"HF vs FastEmbed","text":"<pre><code>import time\nfrom pathlib import Path\nfrom typing import Any, Callable, List, Tuple\n\nimport numpy as np\nimport torch.nn.functional as F\nfrom fastembed.embedding import DefaultEmbedding\nfrom torch import Tensor\nfrom transformers import AutoModel, AutoTokenizer\n</code></pre> <pre><code>documents: List[str] = [\n    \"Chandrayaan-3 is India's third lunar mission\",\n    \"It aimed to land a rover on the Moon's surface - joining the US, China and Russia\",\n    \"The mission is a follow-up to Chandrayaan-2, which had partial success\",\n    \"Chandrayaan-3 will be launched by the Indian Space Research Organisation (ISRO)\",\n    \"The estimated cost of the mission is around $35 million\",\n    \"It will carry instruments to study the lunar surface and atmosphere\",\n    \"Chandrayaan-3 landed on the Moon's surface on 23rd August 2023\",\n    \"It consists of a lander named Vikram and a rover named Pragyan similar to Chandrayaan-2. Its propulsion module would act like an orbiter.\",\n    \"The propulsion module carries the lander and rover configuration until the spacecraft is in a 100-kilometre (62 mi) lunar orbit\",\n    \"The mission used GSLV Mk III rocket for its launch\",\n    \"Chandrayaan-3 was launched from the Satish Dhawan Space Centre in Sriharikota\",\n    \"Chandrayaan-3 was launched earlier in the year 2023\",\n]\n</code></pre> <pre><code>class HF:\n    def __init__(self, model_id: str):\n        self.model = AutoModel.from_pretrained(model_id)\n        self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n\n    def average_pool(self, last_hidden_states: Tensor, attention_mask: Tensor) -&amp;gt; Tensor:\n        last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n        return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n\n    def embed(self, texts: List[str]):\n        # Tokenize the input texts\n        batch_dict = self.tokenizer(texts, max_length=512, padding=True, truncation=True, return_tensors=\"pt\")\n\n        outputs = self.model(**batch_dict)\n        last_hidden_state = model_output[0]\n        # Perform mean pooling with attention weighting\n        input_mask_expanded = np.broadcast_to(np.expand_dims(attention_mask, -1), last_hidden_state.shape)\n        embeddings = np.sum(last_hidden_state * input_mask_expanded, 1) / np.clip(\n            input_mask_expanded.sum(1), a_min=1e-9, a_max=None\n        )\n        embeddings = self.average_pool(outputs.last_hidden_state, batch_dict[\"attention_mask\"])\n\n        return embeddings.detach().numpy()\n\n\nhf = HF(model_id=\"BAAI/bge-small-en\")\n</code></pre> <pre><code>embedding_model = DefaultEmbedding()\n</code></pre> <pre><code>def calculate_time_stats(embed_func: Callable, documents: list, k: int) -&amp;gt; Tuple[float, float, float]:\n    times = []\n\n    for _ in range(k):\n        # Timing the embed_func call\n        start_time = time.time()\n        embeddings = embed_func(documents)\n        end_time = time.time()\n\n        times.append(end_time - start_time)\n\n    # Returning mean, max, and min time for the call\n    return (sum(times) / k, max(times), min(times))\n</code></pre> <pre><code>hf_stats = calculate_time_stats(hf.embed, documents, k=100)\nfst_stats = calculate_time_stats(embedding_model.embed, documents, k=100)\nprint(f\"Average time taken by HF(PyTorch): {hf_stats[0]}\")\nprint(f\"Average time taken by FastEmbed: {fst_stats[0]}\")\n</code></pre> <pre>\n<code>Average time taken by HF(PyTorch): 0.06770326375961304\nAverage time taken by FastEmbed: 2.4080276489257813e-07\n</code>\n</pre> <pre><code>!pip install matplotlib --quiet\n</code></pre> <pre>\n<code>huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n</code>\n</pre> <pre><code>import matplotlib.pyplot as plt\n</code></pre> <pre><code>def plot_character_per_second_comparison(\n    hf_stats: Tuple[float, float, float], fst_stats: Tuple[float, float, float], documents: list\n):\n    # Calculating total characters in documents\n    total_characters = sum(len(doc) for doc in documents)\n\n    # Calculating characters per second for each model\n    hf_chars_per_sec = total_characters / hf_stats[0]  # Mean time is at index 0\n    fst_chars_per_sec = total_characters / fst_stats[0]\n\n    # Plotting the bar chart\n    models = [\"HF Embed (Torch)\", \"FastEmbed\"]\n    chars_per_sec = [hf_chars_per_sec, fst_chars_per_sec]\n\n    bars = plt.bar(models, chars_per_sec, color=[\"#1f356c\", \"#dd1f4b\"])\n    plt.ylabel(\"Characters per Second\")\n    plt.title(\"Characters Processed per Second Comparison\")\n\n    # Adding the number at the top of each bar\n    for bar, chars in zip(bars, chars_per_sec):\n        plt.text(\n            bar.get_x() + bar.get_width() / 2,\n            bar.get_height(),\n            f\"{chars:.1f}\",\n            ha=\"center\",\n            va=\"bottom\",\n            color=\"#1f356c\",\n            fontsize=12,\n        )\n\n    plt.show()\n\n\nplot_character_per_second_comparison(hf_stats, fst_stats, documents)\n</code></pre>"},{"location":"examples/HF_vs_FastEmbed/#huggingface-vs-fastembed","title":"\ud83e\udd17 Huggingface vs \u26a1 FastEmbed\ufe0f","text":"<p>Comparing the performance of Huggingface's \ud83e\udd17 Transformers and \u26a1 FastEmbed\ufe0f on a simple task on the following machine: Apple M2 Max, 32 GB RAM</p>"},{"location":"examples/HF_vs_FastEmbed/#imports","title":"\ud83d\udce6 Imports","text":"<p>Importing the necessary libraries for this comparison.</p>"},{"location":"examples/HF_vs_FastEmbed/#data","title":"\ud83d\udcd6 Data","text":"<p>data is a list of strings, each string is a document.</p>"},{"location":"examples/HF_vs_FastEmbed/#setting-up-huggingface","title":"Setting up \ud83e\udd17 Huggingface","text":"<p>We'll be using the Huggingface Transformers with PyTorch library to generate embeddings. We'll be using the same model across both libraries for a fair(er?) comparison.</p>"},{"location":"examples/HF_vs_FastEmbed/#setting-up-fastembed","title":"Setting up \u26a1\ufe0fFastEmbed","text":"<p>Sorry, don't have a lot to set up here. We'll be using the default model, which is Flag Embedding, same as the Huggingface model.</p>"},{"location":"examples/HF_vs_FastEmbed/#comparison","title":"\ud83d\udcca Comparison","text":"<p>We'll be comparing the following metrics: Minimum, Maximum, Mean, across k runs. Let's write a function to do that:</p>"},{"location":"examples/HF_vs_FastEmbed/#calculating-stats","title":"\ud83d\ude80 Calculating Stats","text":""},{"location":"examples/HF_vs_FastEmbed/#results","title":"\ud83d\udcc8 Results","text":"<p>Let's run the comparison and see the results.</p>"},{"location":"examples/Retrieval_with_FastEmbed/","title":"Retrieval with FastEmbed","text":"<pre><code># !pip install fastembed --quiet --upgrade\n</code></pre> <p>Importing the necessary libraries:</p> <pre><code>from typing import List\nimport numpy as np\nfrom fastembed.embedding import FlagEmbedding as Embedding\n</code></pre> <pre><code># Example list of documents\ndocuments: List[str] = [\n    \"Maharana Pratap was a Rajput warrior king from Mewar\",\n    \"He fought against the Mughal Empire led by Akbar\",\n    \"The Battle of Haldighati in 1576 was his most famous battle\",\n    \"He refused to submit to Akbar and continued guerrilla warfare\",\n    \"His capital was Chittorgarh, which he lost to the Mughals\",\n    \"He died in 1597 at the age of 57\",\n    \"Maharana Pratap is considered a symbol of Rajput resistance against foreign rule\",\n    \"His legacy is celebrated in Rajasthan through festivals and monuments\",\n    \"He had 11 wives and 17 sons, including Amar Singh I who succeeded him as ruler of Mewar\",\n    \"His life has been depicted in various films, TV shows, and books\",\n]\n# Initialize the DefaultEmbedding class with the desired parameters\nembedding_model = Embedding(model_name=\"BAAI/bge-small-en\", max_length=512)\n\n# We'll use the passage_embed method to get the embeddings for the documents\nembeddings: List[np.ndarray] = list(\n    embedding_model.passage_embed(documents)\n)  # notice that we are casting the generator to a list\n\nprint(embeddings[0].shape, len(embeddings))\n</code></pre> <pre>\n<code>(384,) 10\n</code>\n</pre> <pre><code>query = \"Who was Maharana Pratap?\"\nquery_embedding = list(embedding_model.query_embed(query))[0]\nplain_query_embedding = list(embedding_model.embed(query))[0]\n\n\ndef print_top_k(query_embedding, embeddings, documents, k=5):\n    # use numpy to calculate the cosine similarity between the query and the documents\n    scores = np.dot(embeddings, query_embedding)\n    # sort the scores in descending order\n    sorted_scores = np.argsort(scores)[::-1]\n    # print the top 5\n    for i in range(k):\n        print(f\"Rank {i+1}: {documents[sorted_scores[i]]}\")\n</code></pre> <pre><code>print_top_k(query_embedding, embeddings, documents)\n</code></pre> <pre>\n<code>Rank 1: Maharana Pratap was a Rajput warrior king from Mewar\nRank 2: Maharana Pratap is considered a symbol of Rajput resistance against foreign rule\nRank 3: His legacy is celebrated in Rajasthan through festivals and monuments\nRank 4: His capital was Chittorgarh, which he lost to the Mughals\nRank 5: He fought against the Mughal Empire led by Akbar\n</code>\n</pre> <p>Using plain embeddings (from <code>embed</code> method):</p> <pre><code>print_top_k(plain_query_embedding, embeddings, documents)\n</code></pre> <pre>\n<code>Rank 1: He died in 1597 at the age of 57\nRank 2: His life has been depicted in various films, TV shows, and books\nRank 3: Maharana Pratap was a Rajput warrior king from Mewar\nRank 4: He had 11 wives and 17 sons, including Amar Singh I who succeeded him as ruler of Mewar\nRank 5: He fought against the Mughal Empire led by Akbar\n</code>\n</pre> <p>The <code>query_embed</code> is specifically designed for queries, leading to more relevant and context-aware results. The retrieved documents tend to align closely with the query's intent.</p> <p>In contrast, <code>embed</code> is a more general-purpose representation that might not capture the nuances of the query as effectively. The retrieved documents using plain embeddings might be less relevant or ordered differently compared to the results obtained using query embeddings.</p> <p>Conclusion: Using query and passage embeddings leads to more relevant and context-aware results.</p>"},{"location":"examples/Retrieval_with_FastEmbed/#retrieval-with-fastembed","title":"\u2693\ufe0f Retrieval with FastEmbed","text":"<p>This notebook demonstrates how to use FastEmbed to perform vector search and retrieval. It consists of the following sections:</p> <ol> <li>Setup: Installing the necessary packages.</li> <li>Importing Libraries: Importing FastEmbed and other libraries.</li> <li>Data Preparation: Example data and embedding generation.</li> <li>Querying: Defining a function to search documents based on a query.</li> <li>Running Queries: Running example queries.</li> </ol>"},{"location":"examples/Retrieval_with_FastEmbed/#setup","title":"Setup","text":"<p>First, we need to install the dependencies. <code>fastembed</code> to create embeddings and perform retrieval.</p>"},{"location":"examples/Retrieval_with_FastEmbed/#data-preparation","title":"Data Preparation","text":"<p>We initialize the embedding model and generate embeddings for the documents.</p>"},{"location":"examples/Retrieval_with_FastEmbed/#tip-prefer-using-query_embed-for-queries-and-passage_embed-for-documents","title":"\ud83d\udca1 Tip: Prefer using <code>query_embed</code> for queries and <code>passage_embed</code> for documents.","text":""},{"location":"examples/Retrieval_with_FastEmbed/#querying","title":"Querying","text":"<p>We'll define a function to print the top k documents based on a query, and prepare a sample query.</p>"},{"location":"examples/Retrieval_with_FastEmbed/#running-and-comparing-queries","title":"Running and Comparing Queries","text":"<p>Finally, we run our sample query using the <code>print_top_k</code> function.</p> <p>The differences between using query embeddings and plain embeddings can be observed in the retrieved ranks:</p> <p>Using query embeddings (from <code>query_embed</code> method):</p>"},{"location":"experimental/1M_Embedding_Creation/","title":"1M Embedding Creation","text":"<p>Step by Step: 0. Replace for loop with parallel and use an iterable to yield from when working with batch size greater than 10 1. Download the 1M Entities from HF Datasets 2. Embed that -- profile how much time it took</p>"},{"location":"experimental/Binary%20Quantization/","title":"Binary Quantization","text":"<pre><code>!pip install matplotlib tqdm pandas numpy --quiet\n</code></pre> <pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n</code></pre> <pre><code>def get_openai_vectors(force_download: bool = False):\n    res = []\n    for i in tqdm(range(26)):\n        if force_download:\n            !wget https://huggingface.co/api/datasets/KShivendu/dbpedia-entities-openai-1M/parquet/KShivendu--dbpedia-entities-openai-1M/train/{i}.parquet\n        df = pd.read_parquet(f\"{i}.parquet\", engine=\"pyarrow\")\n        res.append(np.stack(df.openai))\n        del df\n\n    openai_vectors = np.concatenate(res)\n    del res\n    return openai_vectors\n\n\nopenai_vectors = get_openai_vectors(force_download=False)\nopenai_vectors.shape\n</code></pre> <pre><code>openai_bin = np.zeros_like(openai_vectors, dtype=np.int8)\nopenai_bin[openai_vectors &amp;gt; 0] = 1\n</code></pre> <pre><code>def accuracy(idx, limit: int, oversampling: int):\n    scores = np.dot(openai_vectors, openai_vectors[idx])\n    dot_results = np.argsort(scores)[-limit:][::-1]\n\n    bin_scores = 1536 - np.logical_xor(openai_bin, openai_bin[idx]).sum(axis=1)\n    bin_results = np.argsort(bin_scores)[-(limit * oversampling) :][::-1]\n\n    return len(set(dot_results).intersection(set(bin_results))) / limit\n</code></pre> <pre><code>number_of_samples = 10\nlimits = [10, 100]\nsampling_rate = [1, 2, 4, 8, 16]\nresults = []\n\ndef mean_accuracy(number_of_samples, limit, sampling_rate):\n    return np.mean([accuracy(i, limit=limit, oversampling=sampling_rate) for i in range(number_of_samples)])\n\nfor i in tqdm(sampling_rate):\n    for j in tqdm(limits):\n        result = {\"sampling_rate\": i, \"limit\": j, \"accuracy\": mean_accuracy(number_of_samples, j, i)}\n        print(result)\n        results.append(result)\n</code></pre> <pre><code>results = pd.DataFrame(results)\nresults\n</code></pre> sampling_rate limit accuracy 1 10 0.800 1 100 0.708 2 10 0.950 2 100 0.877 4 10 0.970 4 100 0.956 8 10 0.990 8 100 0.990 16 10 1.000 16 100 0.998"},{"location":"experimental/Binary%20Quantization/#binary-quantization-of-openai-embedding","title":"Binary Quantization of OpenAI Embedding","text":"<p>In the world of large-scale data retrieval and processing, efficiency is crucial. With the exponential growth of data, the ability to retrieve information quickly and accurately can significantly affect system performance. This blog post explores a technique known as binary quantization applied to OpenAI embeddings, demonstrating how it can enhance retrieval latency by 20x or more.</p>"},{"location":"experimental/Binary%20Quantization/#what-are-openai-embeddings","title":"What Are OpenAI Embeddings?","text":"<p>OpenAI embeddings are numerical representations of textual information. They transform text into a vector space where semantically similar texts are mapped close together. This mathematical representation enables computers to understand and process human language more effectively.</p>"},{"location":"experimental/Binary%20Quantization/#binary-quantization","title":"Binary Quantization","text":"<p>Binary quantization is a method which converts continuous numerical values into binary values (0 or 1). It simplifies the data structure, allowing faster computations. Here's a brief overview of the binary quantization process applied to OpenAI embeddings:</p> <ol> <li>Load Embeddings: OpenAI embeddings are loaded from parquet files.</li> <li>Binary Transformation: The continuous valued vectors are converted into binary form. Here, values greater than 0 are set to 1, and others remain 0.</li> <li>Comparison &amp; Retrieval: Binary vectors are used for comparison using logical XOR operations and other efficient algorithms.</li> </ol>"},{"location":"experimental/Binary%20Quantization/#setup-install-dependencies-imports-download-embeddings","title":"Setup: Install Dependencies, Imports &amp; Download Embeddings","text":""},{"location":"experimental/Binary%20Quantization/#code-walkthrough","title":"\ud83d\udc68\ud83c\udffe\u200d\ud83d\udcbb Code Walkthrough","text":"<p>Here's an explanation of the code structure provided:</p> <ol> <li>Loading Data: OpenAI embeddings are loaded from a parquet files (we can load upto 1M embedding) and concatenated into one array.</li> <li>Binary Conversion: A new array with the same shape is initialized with zeros, and the positive values in the original vectors are set to 1.</li> <li>Accuracy Function: The accuracy function compares original vectors with binary vectors for a given index, limit, and oversampling rate. The comparison is done using dot products and logical XOR, sorting the results, and measuring the intersection.</li> <li>Testing: The accuracy is tested for different oversampling rates (1, 2, 4), revealing a correctness of ~0.96 for an oversampling of 4.</li> </ol>"},{"location":"experimental/Binary%20Quantization/#loading-data","title":"\ud83d\udcbf Loading Data","text":""},{"location":"experimental/Binary%20Quantization/#binary-conversion","title":"\u3193 Binary Conversion","text":"<p>Here, we will use 0 as the threshold for the binary conversion. All values greater than 0 will be set to 1, and others will remain 0. This is a simple and effective way to convert continuous values into binary values for OpenAI embeddings.</p>"},{"location":"experimental/Binary%20Quantization/#accuracy-function","title":"\ud83c\udfaf Accuracy Function","text":"<p>We will use the accuracy function to compare the original vectors with the binary vectors for a given index, limit, and oversampling rate. The comparison is done using dot products and logical XOR, sorting the results, and measuring the intersection.</p>"},{"location":"experimental/Binary%20Quantization/#results","title":"\ud83d\udcca Results","text":""},{"location":"experimental/Binary%20Quantization/#observations","title":"\ud83d\udc40 Observations","text":"<p>As the sampling rate increases, the accuracy generally improves. Higher limits also tend to yield higher accuracy, especially at larger sampling rates. The accuracy reaches 1.0 for a sampling rate of 16 and a limit of 10.</p>"},{"location":"experimental/Binary%20Quantization/#conclusion","title":"\ud83d\ude0e Conclusion","text":"<p>The binary quantization of OpenAI embeddings offers a highly efficient way to improve latency in data retrieval. With a correctness of nearly 97% and an impressive 20x or more improvement in retrieval speed, this technique is a promising avenue for systems dealing with large-scale data.</p>"},{"location":"experimental/FastEmbed_Usage_with_Qdrant/","title":"FastEmbed Usage with Qdrant","text":"<pre><code># !pip install fastembed --quiet --upgrade\n\n# !pip install git+https://github.com/qdrant/qdrant_client.git@dev\n</code></pre> <p>Importing the necessary libraries:</p> <pre><code>from typing import List\nimport numpy as np\nfrom fastembed.embedding import FlagEmbedding as Embedding\nfrom qdrant_client import QdrantClient\n</code></pre> <pre><code># Example list of documents\ndocuments: List[str] = [\n    \"Maharana Pratap was a Rajput warrior king from Mewar\",\n    \"He fought against the Mughal Empire led by Akbar\",\n    \"The Battle of Haldighati in 1576 was his most famous battle\",\n    \"He refused to submit to Akbar and continued guerrilla warfare\",\n    \"His capital was Chittorgarh, which he lost to the Mughals\",\n    \"He died in 1597 at the age of 57\",\n    \"Maharana Pratap is considered a symbol of Rajput resistance against foreign rule\",\n    \"His legacy is celebrated in Rajasthan through festivals and monuments\",\n    \"He had 11 wives and 17 sons, including Amar Singh I who succeeded him as ruler of Mewar\",\n    \"His life has been depicted in various films, TV shows, and books\",\n]\n</code></pre> <p>This tutorial demonstrates how to utilize the QdrantClient to add documents to a collection and query the collection for relevant documents.</p> <pre><code>client = QdrantClient(\":memory:\")\nclient.add(collection_name=\"test_collection\", documents=documents)\n</code></pre> <pre>\n<code>['901ff9d7a90a4e56afe655d1de8e7c06',\n '7bae5aa894164398a9be68d47d72ff7a',\n 'd940bba124e24469ae3166c3110c62f8',\n 'c8dcf956a4f444c6bfe69a00cbeb85ce',\n 'd2aeb24c51f549c5b21851be05cb4048',\n '6d4c672bafef4db68ae72c8986ee8a72',\n 'ff198613768a4361a6d8230b40ee7f78',\n 'a133ad72876b48d48e716533c2d78cf0',\n 'bdf2b016136e4f57b7ac4b65534031b8',\n '5f4f279f304c47839e1a74b0f28de136']</code>\n</pre> <p>These are the ids of the documents we just added. We don't have a use for them in this tutorial, but they can be used to update or delete documents.</p> <pre><code>from qdrant_client.qdrant_fastembed import QueryResponse\n\n\ndef print_top_k_results(results: List[QueryResponse], k: int = 5):\n    print(f\"Top {k} results:\")\n    for i, result in enumerate(results[:k]):\n        print(f\"Rank {i + 1}: {result.document}. Score: {result.score:.2f}\")\n\n\nquery_text = \"Who is Maharana Pratap?\"\nresults = client.query(\n    collection_name=\"test_collection\", query_text=query_text, limit=7\n)  # Returns limit most relevant documents\n\nprint_top_k_results(results)\n</code></pre> <pre>\n<code>Top 5 results:\nRank 1: Maharana Pratap was a Rajput warrior king from Mewar. Score: 0.77\nRank 2: Maharana Pratap is considered a symbol of Rajput resistance against foreign rule. Score: 0.77\nRank 3: His legacy is celebrated in Rajasthan through festivals and monuments. Score: 0.69\nRank 4: He had 11 wives and 17 sons, including Amar Singh I who succeeded him as ruler of Mewar. Score: 0.68\nRank 5: He fought against the Mughal Empire led by Akbar. Score: 0.67\n</code>\n</pre>"},{"location":"experimental/FastEmbed_Usage_with_Qdrant/#experimental-usage-with-qdrant","title":"[Experimental] Usage With Qdrant","text":"<p>&gt; Note: This notebook is experimental and is subject to change. For working with this, use the dev branch of QdrantClient.</p> <p>This notebook demonstrates how to use FastEmbed and Qdrant to perform vector search and retrieval. Qdrant is an open-source vector similarity search engine that is used to store, organize, and query collections of high-dimensional vectors. </p> <p>We will use the Qdrant to add a collection of documents to the engine and then query the collection to retrieve the most relevant documents.</p> <p>It consists of the following sections:</p> <ol> <li>Setup: Installing necessary packages, including the Qdrant Client and FastEmbed.</li> <li>Importing Libraries: Importing FastEmbed and other libraries</li> <li>Data Preparation: Example data and embedding generation</li> <li>Querying: Defining a function to search documents based on a query</li> <li>Running Queries: Running example queries</li> </ol>"},{"location":"experimental/FastEmbed_Usage_with_Qdrant/#setup","title":"Setup","text":"<p>First, we need to install the dependencies. <code>fastembed</code> to create embeddings and perform retrieval, and <code>qdrant-client</code> to interact with the Qdrant database.</p>"},{"location":"experimental/FastEmbed_Usage_with_Qdrant/#data-preparation","title":"Data Preparation","text":"<p>We initialize the embedding model and generate embeddings for the documents.</p>"},{"location":"experimental/FastEmbed_Usage_with_Qdrant/#tip-prefer-using-query_embed-for-queries-and-passage_embed-for-documents","title":"\ud83d\udca1 Tip: Prefer using <code>query_embed</code> for queries and <code>passage_embed</code> for documents.","text":""},{"location":"experimental/FastEmbed_Usage_with_Qdrant/#adding-documents","title":"\u2795 Adding Documents","text":"<p>The <code>add</code> creates a collection if it does not already exist. Now, we can add the documents to the collection:</p>"},{"location":"experimental/FastEmbed_Usage_with_Qdrant/#running-queries","title":"\ud83d\udcdd Running Queries","text":"<p>We'll define a function to print the top k documents based on a query, and prepare a sample query.</p>"},{"location":"experimental/FastEmbed_Usage_with_Qdrant/#conclusion","title":"\ud83c\udfac Conclusion","text":"<p>This tutorial demonstrates the basics of working with the QdrantClient to add and query documents. By following this guide, you can easily integrate Qdrant into your projects for vector similarity search and retrieval.</p> <p>Remember to properly handle the closing of the client connection and further customization of the query parameters according to your specific needs.</p> <p>The official Qdrant Python client documentation can be found here for more details on customization and advanced features.</p>"}]}